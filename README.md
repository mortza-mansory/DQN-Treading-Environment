منطق عملکرد مدل DQN
مدل Deep Q-Network (DQN) یک روش یادگیری تقویتی (Reinforcement Learning) است که از شبکه‌های عصبی عمیق برای تخمین تابع Q (تابع ارزش-عمل) استفاده می‌کند. این مدل در اینجا برای تصمیم‌گیری در معاملات (خرید، فروش، نگهداری) در یک محیط معاملاتی به کار گرفته شده است. منطق عملکرد DQN به شرح زیر است:

محیط و حالت‌ها (Environment and States):
محیط معاملاتی (TradingEnvXY) شامل داده‌های قیمتی (مانند قیمت باز، بسته، بالا، پایین و حجم معاملات) است که از فایل litecoin.xlsx بارگذاری می‌شود.
حالت‌ها (States) شامل ویژگی‌های نرمال‌سازی‌شده (z-score) داده‌های قیمتی هستند که به مدل ورودی داده می‌شوند.
اقدامات (Actions):
سه اقدام گسسته تعریف شده‌اند: نگهداری (Hold)، خرید (Buy)، و فروش (Sell).
این اقدامات در لیست DISCRETE_ACTIONS تعریف شده‌اند و توسط شبکه Q به‌صورت یک شاخص (0 برای نگهداری، 1 برای خرید، 2 برای فروش) انتخاب می‌شوند.
پاداش (Reward):
پاداش بر اساس بازده لگاریتمی (logret) محاسبه می‌شود و با استفاده از تابع money_management اصلاح می‌شود.
پاداش اضافی بر اساس پیش‌بینی قیمت (افزایش یا کاهش 1%) و مدیریت ریسک (Risk Level) تنظیم می‌شود.
اگر معامله سودآور باشد، پاداش مثبت و در غیر این صورت پاداش منفی دریافت می‌شود.
تابع Q و یادگیری:
شبکه Q (QNetwork) یک شبکه عصبی چندلایه است که مقادیر Q را برای هر اقدام در یک حالت خاص تخمین می‌زند.
هدف، یادگیری بهترین اقدام (با بالاترین مقدار Q) برای هر حالت است.
مدل از الگوریتم Experience Replay استفاده می‌کند که تجربیات (state, action, reward, next_state, done) را در یک حافظه (deque) ذخیره کرده و به‌صورت تصادفی نمونه‌هایی را برای آموزش انتخاب می‌کند.
برای پایداری یادگیری، از یک شبکه هدف (Target Network) استفاده می‌شود که به‌صورت دوره‌ای با شبکه اصلی به‌روزرسانی می‌شود (update_target_model).
استراتژی اکتشاف-بهره‌برداری (Exploration-Exploitation):
مدل از استراتژی Epsilon-Greedy استفاده می‌کند. با احتمال epsilon یک اقدام تصادفی انتخاب می‌شود (اکتشاف)، و در غیر این صورت بهترین اقدام بر اساس مقادیر Q انتخاب می‌شود (بهره‌برداری).
مقدار epsilon با نرخ epsilon_decay کاهش می‌یابد تا به epsilon_min برسد، که این باعث کاهش اکتشاف در طول زمان می‌شود.
به‌روزرسانی مدل:
مدل با استفاده از تابع زیان MSE (Mean Squared Error) و بهینه‌ساز Adam آموزش داده می‌شود.
در هر مرحله، مدل مقادیر Q پیش‌بینی‌شده را با مقادیر هدف (پاداش + مقدار Q آینده با ضریب تخفیف gamma) مقایسه کرده و وزن‌های شبکه را به‌روزرسانی می‌کند.
ویژگی‌های محصول (DQN Trading Visualizer)
رابط کاربری تعاملی (UI):
استفاده از Flet برای ایجاد رابط کاربری گرافیکی با قابلیت نمایش نمودارها، معیارها و لاگ‌ها.
شامل کنترل‌هایی مانند دکمه‌های شروع، توقف، مکث، انتخاب مدل، و تنظیمات یادگیری.
نمایش معیارها (Metrics):
نمایش معیارهای کلیدی مانند تعداد خرید، فروش، نگهداری، سود، درصد موفقیت، موجودی کیف پول، دارایی‌ها، معاملات موفق و ناموفق.
این معیارها در زمان واقعی به‌روزرسانی شده و با رنگ‌های مختلف برجسته می‌شوند.
نمودارهای بصری:
نمودار کلی (Chart All): نمایش قیمت واقعی در کل بازه زمانی با یک بازه قرمز برای نشان دادن پنجره فعلی.
نمودار پویا (Dynamic Chart): نمایش قیمت‌های پیش‌بینی‌شده در مقابل قیمت‌های واقعی در یک پنجره متحرک.
نمودار پیش‌بینی (Prediction Chart): نمایش پیش‌بینی قیمت در کل بازه زمانی.
مدیریت ریسک:
کاربران می‌توانند سطح ریسک (خیلی بالا، بالا، متوسط، پایین، خیلی پایین) را انتخاب کنند که درصد سرمایه‌گذاری در هر معامله را تعیین می‌کند.
تابع money_management اطمینان می‌دهد که معاملات با توجه به موجودی و هزینه‌های تراکنش (مانند اسپرد و کارمزد) انجام شوند.
تنظیمات یادگیری:
کاربران می‌توانند پارامترهای DQN مانند gamma (ضریب تخفیف)، epsilon (نرخ اکتشاف)، learning_rate (نرخ یادگیری)، و batch_size را از طریق یک دیالوگ تنظیمات تغییر دهند.
این تنظیمات در فایل learning_settings.json ذخیره می‌شوند.
ذخیره و بارگذاری مدل:
مدل‌های آموزش‌دیده به‌صورت خودکار ذخیره می‌شوند (dqn_model_auto_save.pt) و کاربران می‌توانند مدل‌های ذخیره‌شده را بارگذاری و آزمایش کنند.
نتایج تست شامل نمودارهای مقایسه‌ای و لاگ‌ها در پوشه‌های جداگانه ذخیره می‌شوند.
یادداشت‌ها و درس‌های آموخته‌شده:
یادداشت‌های هوش مصنوعی (AI Notes) و درس‌های آموخته‌شده (Lessons Learned) برای تحلیل اقدامات و پاداش‌ها در طول آموزش و تست نمایش داده می‌شوند.
نحوه یادگیری مدل DQN
آغاز آموزش:
مدل با داده‌های نرمال‌سازی‌شده قیمتی از فایل litecoin.xlsx در بازه زمانی مشخص (مثلاً 2024-01-01 تا 2025-01-01) شروع به کار می‌کند.
محیط معاملاتی (TradingEnvXY) حالت اولیه را فراهم می‌کند، و عامل DQN با استفاده از شبکه Q اقداماتی را انتخاب می‌کند.
انتخاب اقدام:
در هر مرحله، عامل یا یک اقدام تصادفی (بر اساس epsilon) انتخاب می‌کند یا بهترین اقدام را بر اساس مقادیر Q پیش‌بینی‌شده توسط شبکه عصبی انتخاب می‌کند.
اقدامات شامل خرید، فروش یا نگهداری هستند که با توجه به موجودی و سطح ریسک مدیریت می‌شوند.
محاسبه پاداش:
پاداش بر اساس بازده لگاریتمی و پیش‌بینی‌های قیمتی محاسبه می‌شود.
اگر پیش‌بینی قیمت (افزایش یا کاهش 1%) درست باشد، پاداش مثبت دریافت می‌شود؛ در غیر این صورت، پاداش منفی اعمال می‌شود.
تابع money_management پاداش را بر اساس سود یا زیان معامله و هزینه‌های تراکنش اصلاح می‌کند.
ذخیره تجربه:
هر تجربه (شامل حالت فعلی، اقدام، پاداش، حالت بعدی و وضعیت پایان) در حافظه عامل ذخیره می‌شود (agent.remember).
این تجربیات برای آموزش شبکه استفاده می‌شوند.
آموزش شبکه (Replay):
در هر اپیزود، مدل از حافظه نمونه‌های تصادفی انتخاب می‌کند (replay) و شبکه را با استفاده از تابع زیان MSE و مقادیر هدف (پاداش + مقدار Q آینده) به‌روزرسانی می‌کند.
شبکه هدف به‌صورت دوره‌ای به‌روزرسانی می‌شود تا یادگیری پایدارتر شود.
به‌روزرسانی Epsilon:
پس از هر مرحله بازپخش، مقدار epsilon کاهش می‌یابد تا مدل به سمت بهره‌برداری از دانش آموخته‌شده حرکت کند.
نظارت و بازخورد:
معیارها (مانند سود، تعداد معاملات، درصد موفقیت) و نمودارها در زمان واقعی به‌روزرسانی می‌شوند.
درس‌های آموخته‌شده (مانند معاملات ناموفق یا پیش‌بینی‌های نادرست) و یادداشت‌های هوش مصنوعی برای تحلیل عملکرد مدل ثبت می‌شوند.
توقف و ذخیره:
آموزش می‌تواند به‌صورت دستی (توسط دکمه توقف) یا خودکار (در صورت اتمام داده‌ها یا تهی شدن موجودی) متوقف شود.
مدل نهایی ذخیره می‌شود و می‌توان آن را برای تست‌های بعدی بارگذاری کرد.
خلاصه
مدل DQN در این پروژه با استفاده از یادگیری تقویتی و شبکه عصبی عمیق، استراتژی‌های معاملاتی (خرید، فروش، نگهداری) را در یک محیط معاملاتی مبتنی بر داده‌های قیمتی لایت‌کوین یاد می‌گیرد. ویژگی‌های کلیدی شامل رابط کاربری تعامتی، مدیریت ریسک، نمایش معیارها و نمودارها، و امکان تنظیم پارامترهای یادگیری است. یادگیری از طریق تعادل بین اکتشاف و بهره‌برداری، ذخیره تجربیات، و به‌روزرسانی شبکه با استفاده از بازپخش تجربه انجام می‌شود. این سیستم به کاربران امکان می‌دهد عملکرد مدل را نظارت کرده و استراتژی‌های معاملاتی را بهبود بخشند.
