#  Deep Q-Network (DQN) Trading Visualizer

Ù…Ø¯Ù„ **Deep Q-Network (DQN)** ÛŒÚ© Ø±ÙˆØ´ **ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙ‚ÙˆÛŒØªÛŒ (Reinforcement Learning)** Ø§Ø³Øª Ú©Ù‡ Ø§Ø² Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø¹Ù…ÛŒÙ‚ Ø¨Ø±Ø§ÛŒ ØªØ®Ù…ÛŒÙ† ØªØ§Ø¨Ø¹ Ø§Ø±Ø²Ø´-Ø¹Ù…Ù„ (Q-Function) Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.  
Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² DQN Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ **Ø®Ø±ÛŒØ¯ØŒ ÙØ±ÙˆØ´ Ùˆ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ** Ø±Ø§ Ø¯Ø± ÛŒÚ© Ù…Ø­ÛŒØ· Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…ØªÛŒ Ù„Ø§ÛŒØªâ€ŒÚ©ÙˆÛŒÙ† ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.

---

##  Ù…Ù†Ø·Ù‚ Ø¹Ù…Ù„Ú©Ø±Ø¯ DQN

###  Ù…Ø­ÛŒØ· Ùˆ Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ (Environment & States)
- Ù…Ø­ÛŒØ· Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ `TradingEnvXY` Ø´Ø§Ù…Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…ØªÛŒ (OHLCV) Ø§Ø² ÙØ§ÛŒÙ„ **litecoin.xlsx** Ø§Ø³Øª.  
- Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ (States) Ø´Ø§Ù…Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒâ€ŒØ´Ø¯Ù‡ Ø¨Ø§ **z-score** Ù…ÛŒâ€ŒØ¨Ø§Ø´Ù†Ø¯.

###  Ø§Ù‚Ø¯Ø§Ù…Ø§Øª (Actions)
- **Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ (0)** â†’ Hold  
- **Ø®Ø±ÛŒØ¯ (1)** â†’ Buy  
- **ÙØ±ÙˆØ´ (2)** â†’ Sell  

Ø§Ù‚Ø¯Ø§Ù… Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡ ØªÙˆØ³Ø· Ø´Ø¨Ú©Ù‡ Q Ø§Ø² Ù„ÛŒØ³Øª `DISCRETE_ACTIONS` Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

###  Ù¾Ø§Ø¯Ø§Ø´ (Reward)
- Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø§Ø²Ø¯Ù‡ Ù„Ú¯Ø§Ø±ÛŒØªÙ…ÛŒ (**log-return**)  
- Ø§ØµÙ„Ø§Ø­â€ŒØ´Ø¯Ù‡ Ø¨Ø§ ØªØ§Ø¨Ø¹ Ù…Ø¯ÛŒØ±ÛŒØª Ø³Ø±Ù…Ø§ÛŒÙ‡ `money_management`  
- Ù¾Ø§Ø¯Ø§Ø´ Ø§Ø¶Ø§ÙÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³:
  - ØµØ­Øª Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ (Â±1% ØªØºÛŒÛŒØ± Ù‚ÛŒÙ…Øª)  
  - Ø³Ø·Ø­ Ø±ÛŒØ³Ú© Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡ (Risk Level)  
- Ù†ØªÛŒØ¬Ù‡: Ø³ÙˆØ¯ = Ù¾Ø§Ø¯Ø§Ø´ Ù…Ø«Ø¨Øª âœ… | Ø²ÛŒØ§Ù† = Ù¾Ø§Ø¯Ø§Ø´ Ù…Ù†ÙÛŒ âŒ

###  ØªØ§Ø¨Ø¹ Q Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
- Ø´Ø¨Ú©Ù‡ Q (**QNetwork**) Ù…Ù‚Ø§Ø¯ÛŒØ± Q Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ù‚Ø¯Ø§Ù… Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.  
- Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… **Experience Replay**:
  - Ø°Ø®ÛŒØ±Ù‡ `(state, action, reward, next_state, done)` Ø¯Ø± Ø­Ø§ÙØ¸Ù‡ (deque)  
  - Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ ØªØµØ§Ø¯ÙÛŒ Ø§Ø² Ø­Ø§ÙØ¸Ù‡  
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² **Target Network** Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ  
- Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ Ø¨Ø§ `update_target_model`

###  Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø§Ú©ØªØ´Ø§Ù-Ø¨Ù‡Ø±Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ (Exploration vs Exploitation)
- Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… **Epsilon-Greedy**:  
  - Ø¨Ø§ Ø§Ø­ØªÙ…Ø§Ù„ `Îµ` â†’ Ø§Ù‚Ø¯Ø§Ù… ØªØµØ§Ø¯ÙÛŒ (Ø§Ú©ØªØ´Ø§Ù)  
  - Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±Øª â†’ Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù‚Ø¯Ø§Ù… Ø¨Ø± Ø§Ø³Ø§Ø³ Q (Ø¨Ù‡Ø±Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ)  
- `Îµ` Ø¨Ø§ Ù†Ø±Ø® `epsilon_decay` Ú©Ø§Ù‡Ø´ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯ ØªØ§ Ø¨Ù‡ `epsilon_min` Ø¨Ø±Ø³Ø¯.

###  Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…Ø¯Ù„
- ØªØ§Ø¨Ø¹ Ø²ÛŒØ§Ù†: **MSE (Mean Squared Error)**  
- Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²: **Adam**  
- ÙØ±Ù…ÙˆÙ„ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ:  
target = reward + gamma * max(Q(next_state))
---

## ğŸ–¥ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­ØµÙˆÙ„ (Trading Visualizer)

###  Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ (UI)
- Ø³Ø§Ø®ØªÙ‡â€ŒØ´Ø¯Ù‡ Ø¨Ø§ **Flet**  
- Ø´Ø§Ù…Ù„:
- Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§ÛŒ **Ø´Ø±ÙˆØ¹ØŒ ØªÙˆÙ‚ÙØŒ Ù…Ú©Ø«**
- Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ùˆ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
- Ù†Ù…Ø§ÛŒØ´ Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ùˆ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ  

###  Ù†Ù…Ø§ÛŒØ´ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ (Metrics)
- ØªØ¹Ø¯Ø§Ø¯ Ø®Ø±ÛŒØ¯ØŒ ÙØ±ÙˆØ´ØŒ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ  
- Ø³ÙˆØ¯ Ùˆ Ø¯Ø±ØµØ¯ Ù…ÙˆÙÙ‚ÛŒØª  
- Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ú©ÛŒÙ Ù¾ÙˆÙ„ Ùˆ Ø¯Ø§Ø±Ø§ÛŒÛŒâ€ŒÙ‡Ø§  
- Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…ÙˆÙÙ‚/Ù†Ø§Ù…ÙˆÙÙ‚  

###  Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§
- **Chart All** â†’ Ù†Ù…Ø§ÛŒØ´ Ú©Ù„ Ù‚ÛŒÙ…Øª ÙˆØ§Ù‚Ø¹ÛŒ  
- **Dynamic Chart** â†’ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù‚ÛŒÙ…Øª Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡ Ùˆ ÙˆØ§Ù‚Ø¹ÛŒ (Ù¾Ù†Ø¬Ø±Ù‡ Ù…ØªØ­Ø±Ú©)  
- **Prediction Chart** â†’ Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù‚ÛŒÙ…Øª Ø¯Ø± Ú©Ù„ Ø¨Ø§Ø²Ù‡  

###  Ù…Ø¯ÛŒØ±ÛŒØª Ø±ÛŒØ³Ú©
- Ø§Ù†ØªØ®Ø§Ø¨ Ø³Ø·Ø­ Ø±ÛŒØ³Ú©: Ø®ÛŒÙ„ÛŒ Ù¾Ø§ÛŒÛŒÙ† â¬…â¡ Ø®ÛŒÙ„ÛŒ Ø¨Ø§Ù„Ø§  
- Ù…Ø¯ÛŒØ±ÛŒØª Ø³Ø±Ù…Ø§ÛŒÙ‡ Ø¨Ø± Ø§Ø³Ø§Ø³ `money_management`  
- Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ† Ù‡Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ±Ø§Ú©Ù†Ø´ (Ø§Ø³Ù¾Ø±Ø¯ + Ú©Ø§Ø±Ù…Ø²Ø¯)

###  ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
- Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ…:  
- `gamma`, `epsilon`, `learning_rate`, `batch_size`  
- Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± **learning_settings.json**

###  Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
- Ø°Ø®ÛŒØ±Ù‡ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ø¯Ù„ Ø¯Ø±: **dqn_model_auto_save.pt**  
- Ø§Ù…Ú©Ø§Ù† Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ ØªØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ  
- Ø®Ø±ÙˆØ¬ÛŒ ØªØ³Øª Ø´Ø§Ù…Ù„ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ + Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ù…Ø¬Ø²Ø§

###  ÛŒØ§Ø¯Ø¯Ø§Ø´Øªâ€ŒÙ‡Ø§ Ùˆ Ø¯Ø±Ø³â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ®ØªÙ‡â€ŒØ´Ø¯Ù‡
- **AI Notes** â†’ ØªØ­Ù„ÛŒÙ„ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ùˆ Ù¾Ø§Ø¯Ø§Ø´â€ŒÙ‡Ø§  
- **Lessons Learned** â†’ Ø¨Ø§Ø²Ø®ÙˆØ±Ø¯ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…ÙˆÙÙ‚/Ù†Ø§Ù…ÙˆÙÙ‚  

---

##  ÙØ±Ø¢ÛŒÙ†Ø¯ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ DQN

1. **Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´**  
 - Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒâ€ŒØ´Ø¯Ù‡ Ø§Ø² litecoin.xlsx  
 - Ø´Ø±ÙˆØ¹ Ø§Ø² ØªØ§Ø±ÛŒØ® Ù…Ø´Ø®Øµ (Ù…Ø«Ù„Ø§Ù‹ `2024-01-01` ØªØ§ `2025-01-01`)

2. **Ø§Ù†ØªØ®Ø§Ø¨ Ø§Ù‚Ø¯Ø§Ù…**  
 - ØªØµØ§Ø¯ÙÛŒ (Ø§Ú©ØªØ´Ø§Ù) ÛŒØ§ Ø¨Ù‡ØªØ±ÛŒÙ† Q (Ø¨Ù‡Ø±Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ)  
 - Ø§Ù‚Ø¯Ø§Ù…Ø§Øª: Ø®Ø±ÛŒØ¯ØŒ ÙØ±ÙˆØ´ØŒ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ

3. **Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù¾Ø§Ø¯Ø§Ø´**  
 - Ø¨Ø± Ø§Ø³Ø§Ø³ log-return  
 - ØµØ­ÛŒØ­ Ø¨ÙˆØ¯Ù† Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ (Â±1%)  
 - Ø§ØµÙ„Ø§Ø­ ØªÙˆØ³Ø· `money_management`

4. **Ø°Ø®ÛŒØ±Ù‡ ØªØ¬Ø±Ø¨Ù‡**  
 - `(state, action, reward, next_state, done)` â†’ Ø­Ø§ÙØ¸Ù‡ Ø¹Ø§Ù…Ù„

5. **Ø¨Ø§Ø²Ù¾Ø®Ø´ ØªØ¬Ø±Ø¨Ù‡ (Replay)**  
 - Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ ØªØµØ§Ø¯ÙÛŒ Ø§Ø² Ø­Ø§ÙØ¸Ù‡  
 - Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ MSE Ùˆ Ù‡Ø¯Ù Q  
 - Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ Ø´Ø¨Ú©Ù‡ Ù‡Ø¯Ù

6. **Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Epsilon**  
 - Ú©Ø§Ù‡Ø´ ØªØ¯Ø±ÛŒØ¬ÛŒ `Îµ` â†’ Ø­Ø±Ú©Øª Ø¨Ù‡ Ø³Ù…Øª Ø¨Ù‡Ø±Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ

7. **Ù†Ø¸Ø§Ø±Øª Ùˆ Ø¨Ø§Ø²Ø®ÙˆØ±Ø¯**  
 - Ù†Ù…Ø§ÛŒØ´ Ø³ÙˆØ¯ØŒ Ø¯Ø±ØµØ¯ Ù…ÙˆÙÙ‚ÛŒØªØŒ Ù…Ø¹Ø§Ù…Ù„Ø§Øª  
 - Ø«Ø¨Øª AI Notes Ùˆ Lessons Learned  

8. **ØªÙˆÙ‚Ù Ùˆ Ø°Ø®ÛŒØ±Ù‡**  
 - ØªÙˆÙ‚Ù Ø¯Ø³ØªÛŒ ÛŒØ§ Ø®ÙˆØ¯Ú©Ø§Ø± (Ø§ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ / ÙˆØ±Ø´Ú©Ø³ØªÚ¯ÛŒ)  
 - Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ  
---
Ú©Ø¯ Ù†ÙˆÛŒØ³ÛŒ ØªÙˆØ³Ø·: Ù…Ø±ØªØ¶ÛŒ Ù…Ù†ØµÙˆØ±ÛŒ
#  Deep Q-Network (DQN) Trading Visualizer

The **Deep Q-Network (DQN)** is a **Reinforcement Learning** method that uses deep neural networks to approximate the Q-function (action-value function).  
In this project, DQN is used to learn trading strategies **(Buy, Sell, Hold)** in a trading environment based on Litecoin price data.

---

##  DQN Logic

###  Environment & States
- The trading environment `TradingEnvXY` loads OHLCV data from **litecoin.xlsx**.  
- States are normalized features (z-score) of this price data.

###  Actions
- **Hold (0)**  
- **Buy (1)**  
- **Sell (2)**  

Actions are selected by the Q-Network from the list `DISCRETE_ACTIONS`.

###  Reward
- Calculated from **log-return**  
- Adjusted by the money management function `money_management`  
- Extra reward depends on:
  - Prediction accuracy (Â±1% price change)  
  - User-selected **Risk Level**  
- Outcome: Profit = Positive reward âœ… | Loss = Negative reward âŒ

###  Q-Function & Learning
- The Q-Network (**QNetwork**) estimates Q-values for each action.  
- **Experience Replay** algorithm:  
  - Stores `(state, action, reward, next_state, done)` in memory (deque)  
  - Samples random batches for training  
- A **Target Network** is used for stability.  
- Updated periodically with `update_target_model`.

###  Exploration vs Exploitation
- **Epsilon-Greedy strategy**:  
  - With probability `Îµ` â†’ choose a random action (exploration)  
  - Otherwise â†’ choose the best Q-value action (exploitation)  
- `Îµ` decays over time with `epsilon_decay` until `epsilon_min` is reached.

###  Model Update
- Loss function: **MSE (Mean Squared Error)**  
- Optimizer: **Adam**  
- Update rule:
target = reward + gamma * max(Q(next_state))

---

##  Trading Visualizer Features

###  User Interface (UI)
- Built with **Flet**  
- Includes:  
- **Start, Stop, Pause** buttons  
- Model and parameter selection  
- Real-time logs and metrics  

###  Metrics
- Number of Buys, Sells, Holds  
- Profit and success rate  
- Wallet balance and assets  
- Successful / Failed trades  

###  Charts
- **Chart All** â†’ Real price over the full timeline  
- **Dynamic Chart** â†’ Predicted vs. real price in a moving window  
- **Prediction Chart** â†’ Price prediction across the whole dataset  

###  Risk Management
- User can select risk level: Very Low â¬…â¡ Very High  
- Money management ensures correct position sizing  
- Includes transaction costs (spread + fees)

###  Learning Settings
- Adjustable parameters:  
- `gamma`, `epsilon`, `learning_rate`, `batch_size`  
- Saved in **learning_settings.json**

###  Save & Load Model
- Automatic save: **dqn_model_auto_save.pt**  
- Load and test saved models  
- Test results (charts + logs) saved in separate folders

###  AI Notes & Lessons Learned
- **AI Notes** â†’ track actions and rewards  
- **Lessons Learned** â†’ analyze successful and failed trades  

---

##  DQN Training Process

1. **Training Start**  
 - Normalized data from **litecoin.xlsx**  
 - Example range: `2024-01-01` to `2025-01-01`

2. **Action Selection**  
 - Random (exploration) or best Q (exploitation)  
 - Actions: Buy, Sell, Hold

3. **Reward Calculation**  
 - Based on log-return  
 - Bonus if prediction (Â±1%) is correct  
 - Adjusted with `money_management`

4. **Experience Storage**  
 - Save `(state, action, reward, next_state, done)` in agent memory

5. **Replay Training**  
 - Sample random experiences  
 - Train with MSE and Q targets  
 - Periodically update target network  

6. **Epsilon Update**  
 - Gradual decay of `Îµ` â†’ more exploitation over time  

7. **Monitoring & Feedback**  
 - Show profit, success rate, trades  
 - Log AI Notes and Lessons Learned  

8. **Stopping & Saving**  
 - Stop manually or automatically (no data / no balance)  
 - Save final model for later testing  

---

Developed by: **Morteza Mansouri**

